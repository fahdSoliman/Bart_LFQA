{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPCIZe9mJecicvi64m2zwvC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fahdSoliman/Bart_LFQA/blob/main/Replay_base_on_BART_LFQA_with_small_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BART Overview**\n",
        "The Bart model was proposed in BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer on 29 Oct, 2019.\n",
        "\n",
        "# **Long-form question answering**\n",
        "\n"
      ],
      "metadata": {
        "id": "uACignFEa6I4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Long-form Question Answering NLP task**"
      ],
      "metadata": {
        "id": "TydDaM2BT3ks"
      }
    }
  ]
}